hey

I've been thinking about this all afternoon now, and I have a few ideas:

   1. process.on('uncaughtException',...) is unfortunately catch-all, there is no way to cordon off part of the program like you could in a synchronous enviroment by wrapping a block of code in try/catch, and having every error below that point in the stack get caught.

      I've been considering some way to get multiple process objects, including invoking http://github.com/ry/node/blob/master/src/node.js with another a different process object. this idea has potential, it's crazy and thats why i like it, potentially, it would mean you can have another event loop, and so errors thrown in callbacks could get caught by process.on('uncaughtException',...) on a different process object. 

      however, I don't know whats happening with error which originate inside the C parts of node, so they might still get through.

   2. the simple way of providing another process object is just to spawn a child node process and running a test in that. this adds the complication of inter process communication, but should be rock solid. there would be no way for the test subject to monkeypatch the main process or break anything. this seems like a heavy weight solution, but with multi-core and a managed process pool it might be fast anyway.

      I am actually working on this problem already, although I was intending on running each test suite in a separate process rather than each *test*. I've just started, using http://github.com/substack/dnode for the IPC.

   3. don't do anything clever, but make a better error message. if an error is thrown or an assertion is made after test.finish() is called then consider that a error in the test's logic - i.e. not an error in the test subject, but an error in the test. I think this should say, still callback to onSuiteDone but give a report that says the test was invalid due to a assertion/error after test.finished()

idea 3 seems the most realistic, right now, 

anyway, I'm very interested in where you a looking to take this project and would like to help. 
apart from this i'd like to add support for things like giving each test a set time to run in, and being able to treat the test not finishing as some sort of regular error. 

I'd also like to write some tight tests for async_testing, as i belive a testing framework should be well tested!

what features are planning on adding, i'd like to keep my efforts in line with your plans, also!

cheers. Dominic
